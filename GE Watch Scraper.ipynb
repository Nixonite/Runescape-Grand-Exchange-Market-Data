{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "db = MongoClient().rsmarket\n",
    "\n",
    "itemPricePages = range(1,68)\n",
    "itemIndexPages = range(248,266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractItemUrl(soupObj):\n",
    "    source = soupObj.findAll('a',href=True)\n",
    "    itemLinks = []\n",
    "    for link in source:\n",
    "        if re.match('http://www\\.grandexchangewatch\\.com/item/[0-9]+-',link['href']):\n",
    "            itemLinks.append(link['href'])\n",
    "    return itemLinks\n",
    "def extractItemPrices(soupObj):\n",
    "    source = BeautifulSoup(str(soupObj.find('table',{'id':\"data-table\"})))\n",
    "    source = BeautifulSoup(str(source.findAll('tr',{\"class\":re.compile('tr-')})))\n",
    "    source = source.findAll(\"td\")\n",
    "\n",
    "    tempList = []\n",
    "    for i in source:\n",
    "        tempList.append(i.contents[0])\n",
    "\n",
    "    realList = []\n",
    "    for i in range(0,len(tempList),3):\n",
    "        realList.append((tempList[i],tempList[i+1],tempList[i+2]))\n",
    "    return realList\n",
    "def extractItemName(soupObj):\n",
    "    source = soup.findAll(\"h1\")\n",
    "    return source[0].contents[0]['title']\n",
    "\n",
    "def extractItemInfo(soupObj):\n",
    "    source = BeautifulSoup(str(soupObj.findAll('div',{'class':'float-right'})))\n",
    "    source = BeautifulSoup(str(source.findAll('table',{'id':\"data-table\"})))\n",
    "    source = BeautifulSoup(str(source.findAll('tr',{\"class\":re.compile('tr-')})))\n",
    "    source = source.findAll(\"td\")\n",
    "    vals = []\n",
    "    for i in source[12:24][1::2]:\n",
    "        vals.append(i.contents[0])\n",
    "    return vals\n",
    "\n",
    "#extractItemPrices(BeautifulSoup(requests.get(extractItemUrl(soup)[3]).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cc42888a864d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ItemName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextractItemName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractItemInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MembersOnly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-a1e74d496605>\u001b[0m in \u001b[0;36mextractItemName\u001b[0;34m(soupObj)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractItemName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractItemInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoupObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for indexPage in itemIndexPages:\n",
    "    page = requests.get(\"http://www.grandexchangewatch.com/item-db/?query=&sort=popular&dir=desc&show=both&min=&max=&start=\"+str(indexPage)+\"#.VYozi5VVikp\").content\n",
    "    soup = BeautifulSoup(page)\n",
    "    for itemUrl in extractItemUrl(soup)[::2]:#get item urls from index page\n",
    "        document = {}\n",
    "        page = requests.get(itemUrl).content\n",
    "        soup = BeautifulSoup(page)\n",
    "        document['ItemName']=extractItemName(soup)\n",
    "        info = extractItemInfo(soup)\n",
    "        document['MembersOnly']=info[0]\n",
    "        document['DateAdded']=info[1]\n",
    "        document['Category']=info[3]\n",
    "        document['LowAlch']=info[4]\n",
    "        document['HighAlch']=info[5]\n",
    "        priceList = []\n",
    "        time.sleep(0.5)\n",
    "        for pricePage in itemPricePages:\n",
    "            page = requests.get(str(itemUrl)+\"?start=\"+str(pricePage)).content\n",
    "            soup = BeautifulSoup(page)\n",
    "            priceList = priceList+extractItemPrices(soup)\n",
    "            time.sleep(0.5)\n",
    "        document['Prices']=priceList\n",
    "        db.items.insert(document)\n",
    "        print indexPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
